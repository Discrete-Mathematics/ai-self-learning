# 1.神经网络
前置知识：线性回归算法  

主要内容：神经元、感知器、输入层、隐藏层、输出层、激活函数、正向传播、反向传播、随机梯度下降法、全连接神经网络  

推荐资料：[神经网络](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/10.%20Neural%20Network)  
　　　　　[零基础入门深度学习(1) - 感知器](https://www.zybuluo.com/hanbingtao/note/433855)  
　　　　　[零基础入门深度学习(3) - 神经网络和反向传播算法](https://www.zybuluo.com/hanbingtao/note/476663)  
　　　　　[3. 线性神经网络](https://zh-v2.d2l.ai/chapter_linear-networks/index.html)  
　　　　　[4. 多层感知机](https://www.zybuluo.com/hanbingtao/note/433855)  
# 2.卷积神经网络
前置知识：神经网络  

主要内容：输入层、卷积计算层、激励层、池化层、全连接层  

推荐资料：[卷积神经网络](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/11.%20CNN)  
　　　　　[零基础入门深度学习(4) - 卷积神经网络](https://www.zybuluo.com/hanbingtao/note/476663)  
　　　　　[6. 卷积神经网络](https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/index.html)
# 3.循环神经网络
前置知识：神经网络  

主要内容：循环神经网络、双向循环神经网络、深度循环神经网络、BPTT算法  

推荐资料：[循环神经网络](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/12.%20RNN#2-%E5%85%B6%E5%AE%83%E7%B1%BB%E5%9E%8B%E7%9A%84rnn)  
　　　　　[零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458)  
　　　　　[8. 循环神经网络](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/index.html)
# 4.现代循环神经网络
前置知识：循环神经网络  

主要内容：门控循环单元、长短期记忆网络、编码器-解码器架构、序列到序列模型  

推荐资料：[门控循环单元](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/12.1%20GRU)  
　　　　　[长短期记忆](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/12.2%20LSTM)  
　　　　　[零基础入门深度学习(6) - 长短时记忆网络(LSTM)](https://zybuluo.com/hanbingtao/note/581764)  
　　　　　[序列到序列模型](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.5%20seq2seq)  
　　　　　[9. 现代循环神经网络](https://zh-v2.d2l.ai/chapter_recurrent-modern/index.html)
# *5.递归神经网络*
前置知识：循环神经网络  

主要内容：递归神经网络、前向计算、训练  

推荐资料：[零基础入门深度学习(7) - 递归神经网络](https://zybuluo.com/hanbingtao/note/626300)
# 6.注意力机制
前置知识：现代循环神经网络  

主要内容：注意力提示、Nadaraya-Watson 核回归、注意力评分函数、Bahdanau 注意力、多头注意力、自注意力和位置编码、Transformer

推荐资料：[注意力机制](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.6%20Attention)  
　　　　　[Transformer模型](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.7%20Transformer)  
　　　　　[10. 注意力机制](https://zh-v2.d2l.ai/chapter_attention-mechanisms/index.html)  
　　　　　[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
# 7.自然语言处理
前置知识：卷积神经网络、注意力机制  

主要内容：词嵌入、子词嵌入、全局向量词嵌入、textRNN、textCNN、序列到序列模型、BERT、XLNet  

推荐资料：[【自然语言处理笔记】（一）N-gram语言模型与神经网络语言模型](https://zhuanlan.zhihu.com/p/22090476746)  
　　　　　[一起入门语言模型(Language Models)](https://zhuanlan.zhihu.com/p/32292060)  
　　　　　[自然语言处理](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.%20NLP)  
　　　　　[14. 自然语言处理：预训练](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/index.html)
# 8.ChatGPT
前置知识：自然语言处理  

主要内容：GPT、Bert、GPT-2、GPT-3、InstructGPT、ChatGPT  

推荐资料：[一文读懂ChatGPT的前世今生（文末附相关论文下载）](https://zhuanlan.zhihu.com/p/607581437)  
# 9.DeepSeek
前置知识：ChatGPT  

主要内容：DeepSeek LLM、DeepSeekMoE、DeepSeek-V3、DeepSeek-R1  

推荐资料：[deepseek各个版本及论文](https://blog.csdn.net/yiqi1943/article/details/145370544)  
　　　　　[Native Sparse Attention: Hardware-Aligned and Natively
 Trainable Sparse Attention](https://arxiv.org/pdf/2502.11089)
# 10.大模型研究
前置知识：ChatGPT、*DeepSeek*  

主要内容：Prompt、Let's think step by step、In-Context Learning、大模型幻觉、LLM后预训练方法、大模型微调评测、混合小模型、分布式注意力、MoE-Mamba、大模型微调方法、线性注意力、LORA链、实体关系提取  

推荐资料：[关于 ChatGPT 必看的 10 篇论文](https://blog.csdn.net/u010280923/article/details/128969774)  
　　　　　[LLM 大幅提升结果的准确性：Let's think step by step -- 来自论文《Large Language Models are Zero-Shot Reasoners》](https://zhuanlan.zhihu.com/p/652206747)  
　　　　　[2024最新！| 分享10篇优秀论文，涉及大模型微调、Transformer、混合模型等热门话题](https://zhuanlan.zhihu.com/p/677201033)  
# 11.多模态大模型
前置知识：ChatGPT  

主要内容：GPT-4、GPT-4o、o1、GPT-4.5  

推荐资料：[GPT-4论文](https://arxiv.org/pdf/2303.08774)  
　　　　　[]()
# 12.其他大模型
前置知识：ChatGPT  

主要内容：LLaMA、Claude、文心一言、通义千问、豆包、Gemini、Kimi、​腾讯混元、Grok  

推荐资料：[速通LLaMA1：《LLaMA: Open and Efficient Foundation Language Models》全文解读](https://xzl-tech.blog.csdn.net/article/details/142282868)  
　　　　　[速通LLaMA2：《Llama 2: Open Foundation and Fine-Tuned Chat Models》全文解读](https://xzl-tech.blog.csdn.net/article/details/142315279)  
　　　　　[速通LLaMA3：《The Llama 3 Herd of Models》全文解读](https://blog.csdn.net/qq_60735796/article/details/142416119)  
　　　　　[论文笔记Claude 3.5 Haiku和升级版Claude 3.5Sonnet模型](https://zhuanlan.zhihu.com/p/3736787290)  
　　　　　[ERNIE 3.0：文心一言基座模型论文详解](https://zhuanlan.zhihu.com/p/624822605)  
　　　　　[通义千问论文阅读](https://zhuanlan.zhihu.com/p/652625777)  
　　　　　[通义千问(Qwen)2大模型技术报告全文翻译解读](https://blog.csdn.net/sinat_37574187/article/details/142636872#:~:text=%E6%9C%AC%E6%8A%A5%E5%91%8A%E4%BB%8B%E7%BB%8D%E4%BA%86%20Qwen2%20%E7%B3%BB%E5%88%97%EF%BC%8C%E8%BF%99%E6%98%AF%E5%A4%A7%E5%9E%8B%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20%E5%92%8C%E5%A4%A7%E5%9E%8B%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9C%80%E6%96%B0%E6%88%90%E5%91%98%E3%80%82%20%E6%96%87%E6%9C%AC%E5%8F%91%E5%B8%83%E4%BA%86%E4%B8%80%E5%A5%97%E5%85%A8%E9%9D%A2%E7%9A%84%E5%9F%BA%E7%A1%80%E5%92%8C%E6%8C%87%E4%BB%A4%E8%B0%83%E4%BC%98%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%8F%82%E6%95%B0%E8%8C%83%E5%9B%B4%E4%BB%8E%200.5%20%E5%88%B0,%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E3%80%82%20Qwen2%20%E8%B6%85%E8%B6%8A%E4%BA%86%E5%A4%A7%E5%A4%9A%E6%95%B0%E4%BB%A5%E5%89%8D%E7%9A%84%20%E5%BC%80%E6%94%BE%E6%9D%83%E9%87%8D%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%8C%85%E6%8B%AC%E5%85%B6%E5%89%8D%E8%BA%AB%20Qwen1.5%EF%BC%8C%E5%B9%B6%E5%9C%A8%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3%E3%80%81%E7%94%9F%E6%88%90%E3%80%81%E5%A4%9A%E8%AF%AD%E8%A8%80%E8%83%BD%E5%8A%9B%E3%80%81%E7%BC%96%E7%A0%81%E3%80%81%E6%95%B0%E5%AD%A6%E5%92%8C%E6%8E%A8%E7%90%86%E7%AD%89%E5%A4%9A%E4%B8%AA%20%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%20%E4%B8%AD%E5%B1%95%E7%8E%B0%E5%87%BA%E4%B8%8E%E4%B8%93%E6%9C%89%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%AA%B2%E7%BE%8E%E7%9A%84%20%E6%80%A7%E8%83%BD%E3%80%82)  
　　　　　[Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/pdf/2312.11805)  
　　　　　[【论文解读】Kimi-k1.5：无需复杂搜索，Long Context + RL就能实现复杂推理](https://zhuanlan.zhihu.com/p/19612718816)  
　　　　　[腾讯混元大模型核心论文曝光：Scaling law、MoE、合成数据以及更多](https://zhuanlan.zhihu.com/p/20003000733)
# *13.大模型应用开发*
前置知识：*大模型研究*、*多模态大模型*、*强化学习*  

主要内容：Prompt工程、embeddings、向量数据库、RAG、Function Calling、跨模型协作、智能体、Fine-tuning、大模型优化
